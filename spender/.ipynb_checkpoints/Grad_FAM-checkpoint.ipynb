{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99be4e86",
   "metadata": {},
   "source": [
    "## Attention plots - Gradient weighted class activation mapping (Grad - CAM) \n",
    "see figure 4 Melchior22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c602cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from spender import SpectrumEncoder,MLP,encoder_percentiles,load_model\n",
    "import gc\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print('CPU prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,x,y):\n",
    "\n",
    "        \"\"\" generate and organize artificial data from parametrizations of SFHs\"\"\"\n",
    "\n",
    "        self.x=torch.from_numpy(x) #seds\n",
    "        self.y=torch.from_numpy(y) #percentiles\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"total number of samples\"\"\"\n",
    "        return len(self.x[:,0])\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        x=self.x[index,:]\n",
    "        y=self.y[index,:]\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model and dataloader batch again\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_latent=16\n",
    "\n",
    "test_set = Dataset(x_test, y_test[0])\n",
    "print('Shape of the test set: ',np.shape(x_test),np.shape(y_test[0]))\n",
    "#params={'batch_size': len(x_test[:,0]) } #no minitbatches\n",
    "params={'batch_size': 64 } #batch 64\n",
    "test_generator = torch.utils.data.DataLoader(test_set,**params) \n",
    "\n",
    "print('Calling accelerator...')\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "print(accelerator.distributed_type)\n",
    "testloader = accelerator.prepare(test_generator)\n",
    "print(testloader,len(testloader))\n",
    "\n",
    "\n",
    "print('Loading model...')\n",
    "model_file = \"./saved_model/generate_latent_2/latent_\"+str(n_latent)+\"/checkpoint.pt\"\n",
    "model, loss = load_model(model_file, device=accelerator.device,n_hidden=(16,32))\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "\n",
    "#print(model)\n",
    "\n",
    "\n",
    "\n",
    "def grad_fam(model, spec, l_callback):\n",
    "    \n",
    "    # compute attention value and weights\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        h, a = model.encoder._downsample(spec)\n",
    "        a = model.encoder.softmax(a)   \n",
    "    \n",
    "    # compute percentiles, with gradients!\n",
    "    s, y_pred = model._forward(spec)\n",
    "    \n",
    "    # compute specific l\n",
    "    l = l_callback(spec)\n",
    "    l.backward()\n",
    "    \n",
    "    att = a.detach()\n",
    "    \n",
    "    att_grad = model.encoder.attention_grad.detach()\n",
    "    \n",
    "    return att, att_grad\n",
    "    \n",
    "def l_halpha(spec):\n",
    "    sel = (wave > 6560) & (wave < 6566)\n",
    "    diff = spec[:,sel] - 1\n",
    "    return Variable(torch.sum(diff), requires_grad=True)\n",
    "\n",
    "\n",
    "def grad_fam_halpha(model, spec):\n",
    "    return grad_fam(model, spec, l_halpha)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "    \n",
    "batch = next(iter(testloader))\n",
    "\n",
    "print(batch[0].shape) \n",
    "\n",
    "att, att_grad = grad_fam_halpha(model, batch[0].float())\n",
    "    \n",
    "with torch.no_grad():\n",
    "       s, y_pred = model._forward(batch[0].float())\n",
    "    \n",
    "#print(att_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
